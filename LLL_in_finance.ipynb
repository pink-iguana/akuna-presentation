{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2789181",
   "metadata": {},
   "source": [
    "## Can we make use of the LLL algorithm in finance?\n",
    "\n",
    "Maybe?\n",
    "\n",
    "Suppose we have a set of financial instruments whose risk can be represented as a vector in a high-dimensional space. We want to come up with a portfolio with low cardinality that has minimal risk. We can construct a lattice whose basis vectors represent the risk of each instrument. The LLL algorithm can then be used to find a short vector (low risk) in this lattice that is a linear combination of the basis vectors. The coefficients in the linear combination can be interpreted as positions for each instrument in the portfolio.\n",
    "\n",
    "Here, we use 3-year data for the current S&P 500 stocks to construct risk vectors for each stock. The four risk metrics that we have chosen are beta, volatility, Amihud illiquidity (returns divided by volume), and tail risk (correlation with extreme movements in the index)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01546ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import cvxpy as cp\n",
    "from fpylll import IntegerMatrix, LLL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53f3c4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_stock_data(tickers, start_date, end_date):\n",
    "    data = yf.download(tickers, start=start_date, end=end_date)\n",
    "    data = data.dropna(axis=1, how='all')  # Drop columns with all NaN values\n",
    "    data = data.dropna(axis=0, how='all')  # Drop rows with all NaN values\n",
    "    data = data.ffill()  # Fill NaN values with the previous valid observation\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9abf3d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardise_risk_matrix(raw_risk_matrix):\n",
    "    risk_matrix = raw_risk_matrix.copy()\n",
    "    # Min-max scale Volatility and Amihud Illiquidity to [0, 1]\n",
    "    for col in ['Volatility', 'Amihud Illiquidity']:\n",
    "        if col in risk_matrix.columns:\n",
    "            min_val = risk_matrix[col].min()\n",
    "            max_val = risk_matrix[col].max()\n",
    "            risk_matrix[col] = (risk_matrix[col] - min_val) / (max_val - min_val)\n",
    "\n",
    "    return risk_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25844a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_risk_matrix(data):\n",
    "    close_data = data['Close'].dropna(axis=1, how='any')\n",
    "    volume_data = data['Volume'].dropna(axis=1, how='any')\n",
    "    volume_data = volume_data.replace(0, np.nan).ffill()\n",
    "    returns = close_data.pct_change().dropna()  # Calculate daily returns\n",
    "    sp500_returns = returns['^GSPC']  # S&P 500 returns\n",
    "    betas = returns.corrwith(sp500_returns) # Beta of each stock\n",
    "    volatilities = returns.std()  # Volatility of each stock\n",
    "    amihud_illiquidity = ((1e6)*(np.abs(returns)/volume_data).mean())  # Amihud Illiquidity Measure\n",
    "    left_threshold = returns['^GSPC'].quantile(0.05)\n",
    "    right_threshold = returns['^GSPC'].quantile(0.95)\n",
    "    tail_data = returns[(returns['^GSPC'] >= right_threshold) | (returns['^GSPC'] <= left_threshold)]\n",
    "    tail_corr = tail_data.corr()\n",
    "    tail_risk = tail_corr['^GSPC']\n",
    "    raw_risk_matrix = pd.DataFrame({\n",
    "        'Beta': betas,\n",
    "        'Volatility': volatilities,\n",
    "        'Amihud Illiquidity': amihud_illiquidity,\n",
    "        'Tail Risk': tail_risk\n",
    "    })\n",
    "    # standardize the risk matrix using min-max scaling\n",
    "    risk_matrix = standardise_risk_matrix(raw_risk_matrix)\n",
    "    # drop GSPC row\n",
    "    risk_matrix = risk_matrix.drop(index='^GSPC', errors='ignore')\n",
    "    # shuffle order of rows\n",
    "    risk_matrix = risk_matrix.sample(frac=1, random_state=42)\n",
    "\n",
    "    return risk_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aec787f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_stock_combinations_LLL(risk_matrix):\n",
    "    # Scale to integers if needed (LLL requires integer matrix)\n",
    "    scale = 1e6\n",
    "    basis = np.round(risk_matrix.values * scale).astype(int)\n",
    "    n_rows, n_cols = basis.shape\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    pad = np.random.randint(0, 2, size=(n_rows, n_rows - n_cols)) # Random padding with 0s and 1s to make it a square matrix\n",
    "    basis = np.hstack([basis, pad])\n",
    "    R = IntegerMatrix.from_matrix(basis)\n",
    "    U = IntegerMatrix.identity(R.nrows)\n",
    "    LLL.reduction(R, U)\n",
    "    z = np.zeros((risk_matrix.shape[0], risk_matrix.shape[0]), dtype=int)\n",
    "    _ = U.to_matrix(z)\n",
    "    # create a dictionary to map each row index to the corresponding stock combinations and the quantity\n",
    "    stock_combinations = {\n",
    "        i:  {\n",
    "            risk_matrix.index[j]: z[i, j] for j in range(len(z[i])) if z[i, j] != 0\n",
    "        }\n",
    "        for i in range(z.shape[0])\n",
    "    }\n",
    "    return stock_combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b9ce532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Scrape S&P 500 tickers from Wikipedia\n",
    "# url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
    "# table = pd.read_html(url)\n",
    "# sp500 = table[0]\n",
    "# tickers = sp500['Symbol'].tolist()\n",
    "\n",
    "# # convert tickers to yfinance format\n",
    "# tickers = [ticker.replace('.', '-') for ticker in tickers]\n",
    "# tickers = ['^GSPC'] + tickers  # Add S&P 500 index ticker\n",
    "\n",
    "# # Fetch stock data for the S&P 500 companies\n",
    "# start_date = '2020-01-01'\n",
    "# end_date = '2023-01-01'\n",
    "# data = fetch_stock_data(tickers, start_date, end_date)\n",
    "\n",
    "data = pd.read_pickle('sp500_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7eba872",
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_matrix = calculate_risk_matrix(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a98e327",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_combinations_LLL = find_stock_combinations_LLL(risk_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0270a1db",
   "metadata": {},
   "source": [
    "### Convex optimisation\n",
    "\n",
    "For the sake of comparison, we also construct a portfolio (using similar principles/constraints) using convex optimisation. Depending upon the constraints, the convex optimisation routine can take anywhere from a few seconds to a few minutes to run (sometimes returning infinity), while the LLL algorithm always finds a solution in a few seconds. As shown below, both solutions are comparable in finding a portfolio with low cardinality and negligible risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c41585d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_stock_combination_cvx(risk_matrix, max_position=10, min_position=1):\n",
    "    n_stocks = len(risk_matrix)\n",
    "    positions = cp.Variable(n_stocks)\n",
    "    y_pos = cp.Variable(n_stocks, boolean=True)\n",
    "    y_neg = cp.Variable(n_stocks, boolean=True)\n",
    "\n",
    "    # Create risk matrix as numpy array\n",
    "    R = risk_matrix.values\n",
    "\n",
    "    # Objective: Minimize Euclidean norm of risk vector\n",
    "    objective = cp.Minimize(cp.norm(R.T @ positions, 2))\n",
    "\n",
    "    # Constraints\n",
    "    constraints = [\n",
    "        cp.sum(y_pos) >= 1,\n",
    "        cp.sum(y_neg) >= 1,\n",
    "    ]\n",
    "\n",
    "    for i in range(n_stocks):\n",
    "        constraints.append(y_pos[i] + y_neg[i] <= 1)  # Only one of pos/neg per stock\n",
    "\n",
    "        constraints.append(positions[i] >= min_position * y_pos[i] - max_position * (1 - y_pos[i]))\n",
    "        constraints.append(positions[i] <= max_position * y_pos[i])\n",
    "\n",
    "        constraints.append(positions[i] <= -min_position * y_neg[i] + max_position * (1 - y_neg[i]))\n",
    "        constraints.append(positions[i] >= -max_position * y_neg[i])\n",
    "\n",
    "    # Solve problem\n",
    "    prob = cp.Problem(objective, constraints)\n",
    "    prob.solve(solver=cp.ECOS_BB)\n",
    "    print(f\"CVX portfolio risk: {prob.value}\")\n",
    "\n",
    "    stock_combination = {\n",
    "        risk_matrix.index[i]: np.round(positions.value[i]) for i in range(n_stocks)\n",
    "        if np.round(positions.value[i]) != 0\n",
    "    }\n",
    "   \n",
    "    return stock_combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60e5561f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CVX portfolio risk: 4.1284935527842153e-13\n"
     ]
    }
   ],
   "source": [
    "stock_combination_cvx = find_stock_combination_cvx(risk_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cbc218e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All LLL portfolios have risk lower than 7.641257292867465e-05\n"
     ]
    }
   ],
   "source": [
    "LLL_portfolio_risks = [\n",
    "    np.linalg.norm(\n",
    "        np.sum([risk_matrix.loc[s]*stock_combinations_LLL[i][s] for s in stock_combinations_LLL[i].keys()])\n",
    "    )\n",
    "    for i in range(len(stock_combinations_LLL))\n",
    "]\n",
    "\n",
    "print(f\"All LLL portfolios have risk lower than {np.max(LLL_portfolio_risks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "958c7e88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'FICO': np.float64(3.0),\n",
       " 'TYL': np.float64(-2.0),\n",
       " 'TPL': np.float64(-3.0),\n",
       " 'NVR': np.float64(-1.0),\n",
       " 'ERIE': np.float64(10.0),\n",
       " 'AZO': np.float64(-5.0)}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_combination_cvx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16e561d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{344: {'URI': np.int64(-1),\n",
       "  'CHD': np.int64(-1),\n",
       "  'TRV': np.int64(1),\n",
       "  'WFC': np.int64(1),\n",
       "  'TMUS': np.int64(1),\n",
       "  'TER': np.int64(-1),\n",
       "  'BXP': np.int64(1),\n",
       "  'TPR': np.int64(-1),\n",
       "  'HD': np.int64(1),\n",
       "  'CCI': np.int64(-1),\n",
       "  'FICO': np.int64(1),\n",
       "  'EMR': np.int64(-1),\n",
       "  'AON': np.int64(1),\n",
       "  'IDXX': np.int64(-1),\n",
       "  'SBAC': np.int64(-1),\n",
       "  'PARA': np.int64(1)}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the LLL stock combination with the fewest stocks\n",
    "min_stocks = min(len(comb) for comb in stock_combinations_LLL.values())\n",
    "min_combination = {k: v for k, v in stock_combinations_LLL.items() if len(v) == min_stocks}\n",
    "min_combination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bde2aa2",
   "metadata": {},
   "source": [
    "### Limitations:\n",
    "1. The financial instruments might be correlated which is not taken into account here.\n",
    "2. To make the matrix full-rank, we have to add some noise to the risk vectors, otherwise it gives a zero vector.\n",
    "3. It is more likely to pick from among the first few instruments.\n",
    "4. Risk vectors for long positions should look different from those for short positions, which is not considered here.\n",
    "\n",
    "### Advantages of the lattice reduction approach:\n",
    "1. The LLL algorithm is efficient and can handle high-dimensional spaces quite comfortably.\n",
    "2. It can be used to find multiple candidate portfolios with low cardinality with minor tweaks in the setup.\n",
    "3. Unlike convex optimisation, the objective function (the norm of the short vector here) can be non-convex, which might allow for more flexibility in how the risk (basis) vectors are constructed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
